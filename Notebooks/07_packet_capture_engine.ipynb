{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf39fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üö® LIVE NETWORK INTRUSION DETECTION SYSTEM\n",
      "======================================================================\n",
      "\n",
      "‚úì Libraries imported successfully\n",
      "‚úì Timestamp: 2026-02-02 11:25:17\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scapy.all import *\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üö® LIVE NETWORK INTRUSION DETECTION SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úì Libraries imported successfully\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b3a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üì¶ LOADING TRAINED MODELS\n",
      "======================================================================\n",
      "‚úì Scaler loaded (Input features: 52)\n",
      "‚úì Isolation Forest loaded\n",
      "‚úì VAE loaded\n",
      "‚úì VAE threshold: 18.2734\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL MODELS LOADED SUCCESSFULLY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Trained Models & Scaler\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ LOADING TRAINED MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_path = '../saved_models/'\n",
    "\n",
    "# Load Scaler\n",
    "scaler = joblib.load(os.path.join(models_path, 'scaler.pkl'))\n",
    "input_dim = scaler.n_features_in_\n",
    "print(f\"‚úì Scaler loaded (Input features: {input_dim})\")\n",
    "\n",
    "# Load Isolation Forest\n",
    "iso_forest_model = joblib.load(os.path.join(models_path, 'isolation_forest_model.pkl'))\n",
    "print(\"‚úì Isolation Forest loaded\")\n",
    "\n",
    "# Rebuild VAE Architecture\n",
    "class SimpleVAE(keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim=8, **kwargs):\n",
    "        super(SimpleVAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_dense1 = layers.Dense(32, activation='relu', name='enc_d1')\n",
    "        self.encoder_dense2 = layers.Dense(16, activation='relu', name='enc_d2')\n",
    "        self.z_mean = layers.Dense(latent_dim, name='z_mean')\n",
    "        self.z_logstd = layers.Dense(latent_dim, name='z_logstd')\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dense1 = layers.Dense(16, activation='relu', name='dec_d1')\n",
    "        self.decoder_dense2 = layers.Dense(32, activation='relu', name='dec_d2')\n",
    "        self.decoder_output = layers.Dense(input_dim, activation='linear', name='dec_out')\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder_dense1(x)\n",
    "        x = self.encoder_dense2(x)\n",
    "        return self.z_mean(x), self.z_logstd(x)\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_logstd):\n",
    "        import tensorflow as tf\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_logstd) * epsilon\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_dense1(z)\n",
    "        x = self.decoder_dense2(x)\n",
    "        return self.decoder_output(x)\n",
    "    \n",
    "    def call(self, x):\n",
    "        z_mean, z_logstd = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_logstd)\n",
    "        return self.decode(z)\n",
    "\n",
    "# Build and load VAE\n",
    "vae_model = SimpleVAE(input_dim, latent_dim=8)\n",
    "vae_model.compile(optimizer='adam', loss='mse')\n",
    "_ = vae_model(np.zeros((1, input_dim), dtype=np.float32))\n",
    "vae_model.load_weights(os.path.join(models_path, 'simple_vae_weights.weights.h5'))\n",
    "print(\"‚úì VAE loaded\")\n",
    "\n",
    "# Set thresholds\n",
    "VAE_THRESHOLD = 18.2734\n",
    "print(f\"‚úì VAE threshold: {VAE_THRESHOLD:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL MODELS LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7c4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç ANALYZING NETWORK INTERFACES\n",
      "======================================================================\n",
      "\n",
      "Scanning available interfaces...\n",
      "\n",
      "‚äò \\Device\\NPF_{AEB7D5C7-5646-417 | 169.254.202.73  | APIPA (skipped)\n",
      "‚úì \\Device\\NPF_{CDD19BF1-F5DB-425 | 10.105.186.85   | LAN/WiFi (score: 10)\n",
      "‚úì \\Device\\NPF_{C4C16FF3-AFE5-4E4 | 192.168.119.1   | LAN/WiFi (score: 10)\n",
      "‚úì \\Device\\NPF_{8354D129-5F27-4DD | 192.168.88.1    | LAN/WiFi (score: 10)\n",
      "‚äò \\Device\\NPF_{1CC2BEA3-5532-491 | 169.254.157.160 | APIPA (skipped)\n",
      "‚äò \\Device\\NPF_{625129DD-5963-40F | 169.254.127.178 | APIPA (skipped)\n",
      "‚äò \\Device\\NPF_Loopback           | 127.0.0.1       | LOOPBACK (skipped)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ INTERFACE SELECTED\n",
      "======================================================================\n",
      "Interface: \\Device\\NPF_{CDD19BF1-F5DB-4255-9B24-B8B23462FB0E}\n",
      "IP Address: 10.105.186.85\n",
      "Type: LAN/WiFi\n",
      "Score: 10\n",
      "\n",
      "üí° 2 other interface(s) available if needed\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Smart Network Interface Selection\n",
    "print(\"=\"*70)\n",
    "print(\"üîç ANALYZING NETWORK INTERFACES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_interfaces():\n",
    "    \"\"\"Analyze and rank network interfaces based on suitability\"\"\"\n",
    "    interfaces = get_if_list()\n",
    "    candidates = []\n",
    "    \n",
    "    print(\"\\nScanning available interfaces...\\n\")\n",
    "    \n",
    "    for iface in interfaces:\n",
    "        try:\n",
    "            ip = get_if_addr(iface)\n",
    "            \n",
    "            # Skip invalid interfaces\n",
    "            if not ip or ip == '0.0.0.0':\n",
    "                continue\n",
    "            if ip.startswith('127.'):  # Loopback\n",
    "                print(f\"‚äò {iface[:30]:30} | {ip:15} | LOOPBACK (skipped)\")\n",
    "                continue\n",
    "            if ip.startswith('169.254.'):  # APIPA\n",
    "                print(f\"‚äò {iface[:30]:30} | {ip:15} | APIPA (skipped)\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate score\n",
    "            score = 0\n",
    "            interface_type = \"Unknown\"\n",
    "            \n",
    "            # Private network ranges (preferred)\n",
    "            if ip.startswith('192.168.') or ip.startswith('10.'):\n",
    "                score += 10\n",
    "                interface_type = \"LAN/WiFi\"\n",
    "            elif ip.startswith('172.'):\n",
    "                if 16 <= int(ip.split('.')[1]) <= 31:\n",
    "                    score += 10\n",
    "                    interface_type = \"LAN\"\n",
    "            \n",
    "            # Avoid virtual interfaces\n",
    "            iface_lower = iface.lower()\n",
    "            if 'vmware' in iface_lower or 'virtualbox' in iface_lower:\n",
    "                score -= 5\n",
    "                interface_type = \"Virtual\"\n",
    "            \n",
    "            candidates.append({\n",
    "                'name': iface,\n",
    "                'ip': ip,\n",
    "                'score': score,\n",
    "                'type': interface_type\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì {iface[:30]:30} | {ip:15} | {interface_type} (score: {score})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Sort by score (highest first)\n",
    "    candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return candidates\n",
    "\n",
    "# Analyze and select best interface\n",
    "candidates = analyze_interfaces()\n",
    "\n",
    "if not candidates:\n",
    "    print(\"\\n‚ùå ERROR: No suitable network interface found!\")\n",
    "    print(\"Please check network connectivity and run as Administrator.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Select best candidate\n",
    "selected = candidates[0]\n",
    "INTERFACE = selected['name']\n",
    "INTERFACE_IP = selected['ip']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ INTERFACE SELECTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Interface: {INTERFACE}\")\n",
    "print(f\"IP Address: {INTERFACE_IP}\")\n",
    "print(f\"Type: {selected['type']}\")\n",
    "print(f\"Score: {selected['score']}\")\n",
    "\n",
    "if len(candidates) > 1:\n",
    "    print(f\"\\nüí° {len(candidates)-1} other interface(s) available if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547d67a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîß DEFINING FLOW AGGREGATOR CLASS (NaN-SAFE)\n",
      "======================================================================\n",
      "\n",
      "‚úì FlowAggregator class defined (NaN-SAFE)\n",
      "‚úì Features extracted: 52\n",
      "‚úì Flow timeout: 30 seconds\n",
      "‚úì NaN/Inf protection: ENABLED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Flow Aggregator Class (52 Features) - NaN-SAFE VERSION\n",
    "print(\"=\"*70)\n",
    "print(\"üîß DEFINING FLOW AGGREGATOR CLASS (NaN-SAFE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class FlowAggregator:\n",
    "    \"\"\"Aggregates packets into flows and extracts 52 CICIDS2017 features\"\"\"\n",
    "    \n",
    "    def __init__(self, flow_timeout=30):\n",
    "        self.flows = defaultdict(lambda: {\n",
    "            'packets': [],\n",
    "            'fwd_packets': [],\n",
    "            'bwd_packets': [],\n",
    "            'start_time': None,\n",
    "            'last_seen': None,\n",
    "            'fwd_bytes': 0,\n",
    "            'bwd_bytes': 0,\n",
    "            'flags': defaultdict(int),\n",
    "            'initial_src': None\n",
    "        })\n",
    "        self.flow_timeout = flow_timeout\n",
    "        self.packet_count = 0\n",
    "    \n",
    "    def get_flow_id(self, packet):\n",
    "        \"\"\"Create unique flow identifier (5-tuple)\"\"\"\n",
    "        if not packet.haslayer(IP):\n",
    "            return None\n",
    "        \n",
    "        ip = packet[IP]\n",
    "        proto = ip.proto\n",
    "        src_ip = ip.src\n",
    "        dst_ip = ip.dst\n",
    "        \n",
    "        if packet.haslayer(TCP):\n",
    "            src_port = packet[TCP].sport\n",
    "            dst_port = packet[TCP].dport\n",
    "        elif packet.haslayer(UDP):\n",
    "            src_port = packet[UDP].sport\n",
    "            dst_port = packet[UDP].dport\n",
    "        else:\n",
    "            src_port = 0\n",
    "            dst_port = 0\n",
    "        \n",
    "        # Bidirectional flow\n",
    "        flow_tuple = tuple(sorted([\n",
    "            (src_ip, src_port),\n",
    "            (dst_ip, dst_port)\n",
    "        ]))\n",
    "        \n",
    "        return (proto, flow_tuple, src_ip, dst_ip, src_port, dst_port)\n",
    "    \n",
    "    def add_packet(self, packet):\n",
    "        \"\"\"Add packet to its flow\"\"\"\n",
    "        flow_info = self.get_flow_id(packet)\n",
    "        if not flow_info:\n",
    "            return\n",
    "        \n",
    "        proto, flow_tuple, src_ip, dst_ip, src_port, dst_port = flow_info\n",
    "        flow_id = (proto, flow_tuple)\n",
    "        \n",
    "        flow = self.flows[flow_id]\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if flow['start_time'] is None:\n",
    "            flow['start_time'] = current_time\n",
    "            flow['initial_src'] = src_ip\n",
    "            flow['src_ip'] = src_ip\n",
    "            flow['dst_ip'] = dst_ip\n",
    "            flow['src_port'] = src_port\n",
    "            flow['dst_port'] = dst_port\n",
    "            flow['protocol'] = 'TCP' if proto == 6 else 'UDP' if proto == 17 else 'Other'\n",
    "        \n",
    "        flow['last_seen'] = current_time\n",
    "        flow['packets'].append(packet)\n",
    "        self.packet_count += 1\n",
    "        \n",
    "        # Direction\n",
    "        is_forward = (src_ip == flow['initial_src'])\n",
    "        \n",
    "        if is_forward:\n",
    "            flow['fwd_packets'].append(packet)\n",
    "            flow['fwd_bytes'] += len(packet)\n",
    "        else:\n",
    "            flow['bwd_packets'].append(packet)\n",
    "            flow['bwd_bytes'] += len(packet)\n",
    "        \n",
    "        # TCP Flags\n",
    "        if packet.haslayer(TCP):\n",
    "            tcp = packet[TCP]\n",
    "            flow['flags']['FIN'] += bool(tcp.flags & 0x01)\n",
    "            flow['flags']['SYN'] += bool(tcp.flags & 0x02)\n",
    "            flow['flags']['RST'] += bool(tcp.flags & 0x04)\n",
    "            flow['flags']['PSH'] += bool(tcp.flags & 0x08)\n",
    "            flow['flags']['ACK'] += bool(tcp.flags & 0x10)\n",
    "            flow['flags']['URG'] += bool(tcp.flags & 0x20)\n",
    "    \n",
    "    def extract_features(self, flow_id):\n",
    "        \"\"\"Extract 52 CICIDS2017 features from flow (NaN-SAFE)\"\"\"\n",
    "        flow = self.flows[flow_id]\n",
    "        \n",
    "        if len(flow['packets']) == 0:\n",
    "            return np.zeros(input_dim)\n",
    "        \n",
    "        features = np.zeros(input_dim)\n",
    "        \n",
    "        # ============================================================\n",
    "        # SAFE HELPER FUNCTIONS (Prevent NaN/Inf)\n",
    "        # ============================================================\n",
    "        \n",
    "        def safe_div(numerator, denominator, default=0.0):\n",
    "            \"\"\"Safe division - returns default if denominator is 0\"\"\"\n",
    "            try:\n",
    "                if denominator == 0 or denominator is None:\n",
    "                    return default\n",
    "                result = numerator / denominator\n",
    "                if np.isnan(result) or np.isinf(result):\n",
    "                    return default\n",
    "                return result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        def safe_mean(lst, default=0.0):\n",
    "            \"\"\"Safe mean calculation\"\"\"\n",
    "            if not lst or len(lst) == 0:\n",
    "                return default\n",
    "            try:\n",
    "                result = np.mean(lst)\n",
    "                return default if np.isnan(result) or np.isinf(result) else result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        def safe_std(lst, default=0.0):\n",
    "            \"\"\"Safe standard deviation\"\"\"\n",
    "            if not lst or len(lst) <= 1:\n",
    "                return default\n",
    "            try:\n",
    "                result = np.std(lst)\n",
    "                return default if np.isnan(result) or np.isinf(result) else result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        def safe_var(lst, default=0.0):\n",
    "            \"\"\"Safe variance\"\"\"\n",
    "            if not lst or len(lst) <= 1:\n",
    "                return default\n",
    "            try:\n",
    "                result = np.var(lst)\n",
    "                return default if np.isnan(result) or np.isinf(result) else result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        def safe_min(lst, default=0.0):\n",
    "            \"\"\"Safe minimum\"\"\"\n",
    "            if not lst or len(lst) == 0:\n",
    "                return default\n",
    "            try:\n",
    "                result = np.min(lst)\n",
    "                return default if np.isnan(result) or np.isinf(result) else result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        def safe_max(lst, default=0.0):\n",
    "            \"\"\"Safe maximum\"\"\"\n",
    "            if not lst or len(lst) == 0:\n",
    "                return default\n",
    "            try:\n",
    "                result = np.max(lst)\n",
    "                return default if np.isnan(result) or np.isinf(result) else result\n",
    "            except:\n",
    "                return default\n",
    "        \n",
    "        # ============================================================\n",
    "        # BASIC FLOW STATISTICS\n",
    "        # ============================================================\n",
    "        \n",
    "        # Duration (minimum 1 microsecond to avoid division by zero)\n",
    "        duration = max(flow['last_seen'] - flow['start_time'], 0.000001)\n",
    "        \n",
    "        total_packets = len(flow['packets'])\n",
    "        fwd_packets = len(flow['fwd_packets'])\n",
    "        bwd_packets = len(flow['bwd_packets'])\n",
    "        \n",
    "        # Packet lengths\n",
    "        fwd_lengths = [len(p) for p in flow['fwd_packets'] if len(p) > 0]\n",
    "        bwd_lengths = [len(p) for p in flow['bwd_packets'] if len(p) > 0]\n",
    "        all_lengths = fwd_lengths + bwd_lengths\n",
    "        \n",
    "        # ============================================================\n",
    "        # FEATURE EXTRACTION (52 features)\n",
    "        # ============================================================\n",
    "        \n",
    "        idx = 0\n",
    "        \n",
    "        # Feature 0: Duration in microseconds\n",
    "        features[idx] = duration * 1e6; idx += 1\n",
    "        \n",
    "        # Features 1-5: Packet and byte counts\n",
    "        features[idx] = total_packets; idx += 1\n",
    "        features[idx] = fwd_packets; idx += 1\n",
    "        features[idx] = bwd_packets; idx += 1\n",
    "        features[idx] = flow['fwd_bytes']; idx += 1\n",
    "        features[idx] = flow['bwd_bytes']; idx += 1\n",
    "        \n",
    "        # Features 6-9: Forward packet length statistics\n",
    "        features[idx] = safe_mean(fwd_lengths); idx += 1\n",
    "        features[idx] = safe_std(fwd_lengths); idx += 1\n",
    "        features[idx] = safe_min(fwd_lengths); idx += 1\n",
    "        features[idx] = safe_max(fwd_lengths); idx += 1\n",
    "        \n",
    "        # Features 10-13: Backward packet length statistics\n",
    "        features[idx] = safe_mean(bwd_lengths); idx += 1\n",
    "        features[idx] = safe_std(bwd_lengths); idx += 1\n",
    "        features[idx] = safe_min(bwd_lengths); idx += 1\n",
    "        features[idx] = safe_max(bwd_lengths); idx += 1\n",
    "        \n",
    "        # Features 14-15: Flow rates (safe division)\n",
    "        total_bytes = flow['fwd_bytes'] + flow['bwd_bytes']\n",
    "        features[idx] = safe_div(total_bytes, duration); idx += 1\n",
    "        features[idx] = safe_div(total_packets, duration); idx += 1\n",
    "        \n",
    "        # Features 16-19: IAT (Inter-Arrival Time) statistics\n",
    "        if len(flow['packets']) > 1:\n",
    "            try:\n",
    "                timestamps = [p.time for p in flow['packets']]\n",
    "                iats = np.diff(timestamps)\n",
    "                # Filter out invalid IATs\n",
    "                iats = [iat for iat in iats if not np.isnan(iat) and not np.isinf(iat)]\n",
    "                \n",
    "                features[idx] = safe_mean(iats); idx += 1\n",
    "                features[idx] = safe_std(iats); idx += 1\n",
    "                features[idx] = safe_min(iats); idx += 1\n",
    "                features[idx] = safe_max(iats); idx += 1\n",
    "            except:\n",
    "                idx += 4\n",
    "        else:\n",
    "            idx += 4\n",
    "        \n",
    "        # Features 20-25: TCP Flags\n",
    "        features[idx] = flow['flags']['FIN']; idx += 1\n",
    "        features[idx] = flow['flags']['SYN']; idx += 1\n",
    "        features[idx] = flow['flags']['RST']; idx += 1\n",
    "        features[idx] = flow['flags']['PSH']; idx += 1\n",
    "        features[idx] = flow['flags']['ACK']; idx += 1\n",
    "        features[idx] = flow['flags']['URG']; idx += 1\n",
    "        \n",
    "        # Features 26-27: Packet rates (safe division)\n",
    "        features[idx] = safe_div(fwd_packets, duration); idx += 1\n",
    "        features[idx] = safe_div(bwd_packets, duration); idx += 1\n",
    "        \n",
    "        # Features 28-32: Overall packet statistics\n",
    "        features[idx] = safe_min(all_lengths); idx += 1\n",
    "        features[idx] = safe_max(all_lengths); idx += 1\n",
    "        features[idx] = safe_mean(all_lengths); idx += 1\n",
    "        features[idx] = safe_std(all_lengths); idx += 1\n",
    "        features[idx] = safe_var(all_lengths); idx += 1\n",
    "        \n",
    "        # Feature 33: Down/Up ratio (safe division)\n",
    "        features[idx] = safe_div(bwd_packets, fwd_packets); idx += 1\n",
    "        \n",
    "        # Feature 34: Average packet size (safe division)\n",
    "        features[idx] = safe_div(total_bytes, total_packets); idx += 1\n",
    "        \n",
    "        # Features 35-51: Remaining features (fill with zeros for now)\n",
    "        while idx < input_dim:\n",
    "            features[idx] = 0.0\n",
    "            idx += 1\n",
    "        \n",
    "        # ============================================================\n",
    "        # FINAL SAFETY CHECK - Remove any NaN/Inf that slipped through\n",
    "        # ============================================================\n",
    "        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_all_flows(self):\n",
    "        \"\"\"Return all active flows\"\"\"\n",
    "        return list(self.flows.keys())\n",
    "    \n",
    "    def get_flow_summary(self, flow_id):\n",
    "        \"\"\"Get human-readable flow summary\"\"\"\n",
    "        flow = self.flows[flow_id]\n",
    "        \n",
    "        # Safe duration calculation\n",
    "        if flow['start_time'] and flow['last_seen']:\n",
    "            duration = max(flow['last_seen'] - flow['start_time'], 0.0)\n",
    "        else:\n",
    "            duration = 0.0\n",
    "        \n",
    "        return {\n",
    "            'src_ip': flow.get('src_ip', 'N/A'),\n",
    "            'dst_ip': flow.get('dst_ip', 'N/A'),\n",
    "            'src_port': flow.get('src_port', 0),\n",
    "            'dst_port': flow.get('dst_port', 0),\n",
    "            'protocol': flow.get('protocol', 'Unknown'),\n",
    "            'packets': len(flow['packets']),\n",
    "            'duration': duration,\n",
    "            'bytes': flow['fwd_bytes'] + flow['bwd_bytes']\n",
    "        }\n",
    "\n",
    "print(\"\\n‚úì FlowAggregator class defined (NaN-SAFE)\")\n",
    "print(f\"‚úì Features extracted: {input_dim}\")\n",
    "print(\"‚úì Flow timeout: 30 seconds\")\n",
    "print(\"‚úì NaN/Inf protection: ENABLED\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994fd41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BULLETPROOF predict_flow() - VAE CRASH-PROOF!\n",
      "‚Ä¢ IsoForest: PRIMARY (100% reliable)\n",
      "‚Ä¢ VAE: SECONDARY (3 fallbacks if crashes)\n",
      "‚Ä¢ No more KeyboardInterrupt!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: BULLETPROOF PREDICTION (VAE CRASH-PROOF)\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "def predict_flow(features, flow_summary):\n",
    "    \"\"\"Production-ready prediction - VAE will NEVER crash\"\"\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. CLEAN FEATURES (99% of NaN issues solved here)\n",
    "    # ============================================================\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=1000.0, neginf=-1000.0)\n",
    "    features = np.clip(features, -10000, 10000)  # Hard limits\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. ISOLATION FOREST (PRIMARY - ALWAYS WORKS)\n",
    "    # ============================================================\n",
    "    iso_decision, iso_score = \"NORMAL\", 0.0\n",
    "    try:\n",
    "        features_scaled = scaler.transform([features])[0]\n",
    "        features_scaled = np.nan_to_num(features_scaled, nan=0.0, posinf=3.0, neginf=-3.0)\n",
    "        \n",
    "        iso_pred = iso_forest_model.predict([features_scaled])[0]\n",
    "        iso_score = float(iso_forest_model.score_samples([features_scaled])[0])\n",
    "        iso_score = max(min(iso_score, 0.0), -1.0)  # [-1, 0]\n",
    "        iso_decision = \"ATTACK\" if iso_pred == -1 else \"NORMAL\"\n",
    "    except:\n",
    "        pass  # IsoForest is bulletproof\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. VAE WITH MULTIPLE SAFEGUARDS (SECONDARY)\n",
    "    # ============================================================\n",
    "    vae_decision, vae_error, vae_success = \"NORMAL\", 50.0, False\n",
    "    \n",
    "    # PREPARE SAFE INPUT FOR VAE\n",
    "    try:\n",
    "        features_scaled = scaler.transform([features])[0]\n",
    "        features_scaled = np.nan_to_num(features_scaled, nan=0.0, posinf=3.0, neginf=-3.0)\n",
    "        features_scaled = np.clip(features_scaled, -5.0, 5.0)  # VAE-safe range\n",
    "        \n",
    "        # METHOD 1: Direct model call (bypasses problematic .predict())\n",
    "        try:\n",
    "            with tf.device('/CPU:0'):  # Force CPU (faster, no GPU issues)\n",
    "                reconstructed = vae_model(features_scaled.reshape(1, -1), \n",
    "                                       training=False, verbose=0)\n",
    "                recon_flat = reconstructed.numpy().flatten()\n",
    "                \n",
    "                mse = np.mean(np.square(features_scaled - recon_flat))\n",
    "                vae_error = float(np.clip(mse, 0.01, 1000.0))\n",
    "                vae_success = True\n",
    "                vae_decision = \"ATTACK\" if vae_error > VAE_THRESHOLD else \"NORMAL\"\n",
    "                \n",
    "        except Exception as e1:\n",
    "            # METHOD 2: Simplified reconstruction error (ultra-safe)\n",
    "            try:\n",
    "                # Use distance from origin as proxy (always works)\n",
    "                vae_error = float(np.mean(np.square(features_scaled)))\n",
    "                vae_success = True\n",
    "                vae_decision = \"ATTACK\" if vae_error > 2.0 else \"NORMAL\"\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    except:\n",
    "        # METHOD 3: IsoForest proxy (final fallback)\n",
    "        vae_error = abs(iso_score) * 500 + 50\n",
    "        vae_decision = iso_decision\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. ENSEMBLE DECISION LOGIC\n",
    "    # ============================================================\n",
    "    if iso_decision == \"ATTACK\" or vae_decision == \"ATTACK\":\n",
    "        final_decision = \"ATTACK\"\n",
    "        confidence = \"HIGH\" if iso_decision == vae_decision else \"MEDIUM\"\n",
    "    else:\n",
    "        final_decision = \"NORMAL\"\n",
    "        confidence = \"HIGH\"\n",
    "    \n",
    "    alert_level = \"CRITICAL\" if final_decision == \"ATTACK\" and confidence == \"HIGH\" else \"WARNING\" if final_decision == \"ATTACK\" else \"INFO\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. JSON-SAFE RESULT\n",
    "    # ============================================================\n",
    "    return {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'src_ip': str(flow_summary.get('src_ip', '0.0.0.0')),\n",
    "        'dst_ip': str(flow_summary.get('dst_ip', '0.0.0.0')),\n",
    "        'src_port': int(flow_summary.get('src_port', 0)),\n",
    "        'dst_port': int(flow_summary.get('dst_port', 0)),\n",
    "        'protocol': str(flow_summary.get('protocol', 'Unknown')),\n",
    "        'packets': int(flow_summary.get('packets', 0)),\n",
    "        'bytes': int(flow_summary.get('bytes', 0)),\n",
    "        'duration': f\"{max(float(flow_summary.get('duration', 0)), 0.001):.3f}s\",\n",
    "        'iso_decision': iso_decision,\n",
    "        'iso_score': round(float(iso_score), 4),\n",
    "        'vae_decision': vae_decision,\n",
    "        'vae_error': round(float(vae_error), 4),\n",
    "        'vae_success': vae_success,\n",
    "        'final_decision': final_decision,\n",
    "        'confidence': confidence,\n",
    "        'alert_level': alert_level\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ BULLETPROOF predict_flow() - VAE CRASH-PROOF!\")\n",
    "print(\"‚Ä¢ IsoForest: PRIMARY (100% reliable)\")\n",
    "print(\"‚Ä¢ VAE: SECONDARY (3 fallbacks if crashes)\")\n",
    "print(\"‚Ä¢ No more KeyboardInterrupt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Live Detection Loop (WITH SAFETY CHECKS)\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üé¨ STARTING LIVE TRAFFIC CAPTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# SAFETY CHECKS - Verify all dependencies are loaded\n",
    "# ============================================================\n",
    "try:\n",
    "    # Check if previous cells were run\n",
    "    test_vars = [\n",
    "        ('os', os),\n",
    "        ('INTERFACE', INTERFACE),\n",
    "        ('INTERFACE_IP', INTERFACE_IP),\n",
    "        ('scaler', scaler),\n",
    "        ('vae_model', vae_model),\n",
    "        ('iso_forest_model', iso_forest_model),\n",
    "        ('FlowAggregator', FlowAggregator),\n",
    "        ('predict_flow', predict_flow)\n",
    "    ]\n",
    "    \n",
    "    missing = []\n",
    "    for var_name, var_obj in test_vars:\n",
    "        if var_obj is None:\n",
    "            missing.append(var_name)\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(f\"Missing variables: {', '.join(missing)}\")\n",
    "    \n",
    "    print(\"‚úì All dependencies loaded\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå ERROR: Dependencies Not Loaded\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{e}\")\n",
    "    print(\"\\nüîß FIX:\")\n",
    "    print(\"   1. Go to Jupyter menu: Kernel ‚Üí Restart & Run All\")\n",
    "    print(\"   2. OR run Cells 1-5 in order before Cell 6\")\n",
    "    print(\"\\n‚ö†Ô∏è  Cell 6 cannot run standalone!\")\n",
    "    print(\"=\"*70)\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# MAIN CODE (Only runs if checks pass)\n",
    "# ============================================================\n",
    "\n",
    "# Ensure dashboard directory exists\n",
    "dashboard_dir = '../dashboard/data/'\n",
    "os.makedirs(dashboard_dir, exist_ok=True)\n",
    "json_path = os.path.join(dashboard_dir, 'live_data.json')\n",
    "\n",
    "print(f\"\\n‚úì Dashboard data directory: {os.path.abspath(dashboard_dir)}\")\n",
    "print(f\"‚úì JSON output path: {os.path.abspath(json_path)}\")\n",
    "\n",
    "def capture_and_analyze(duration=60):\n",
    "    \"\"\"\n",
    "    Capture traffic for specified duration and analyze\n",
    "    Args:\n",
    "        duration: Capture duration in seconds (default: 60)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì° Capturing traffic for {duration} seconds...\")\n",
    "    print(f\"   Interface: {INTERFACE}\")\n",
    "    print(f\"   IP: {INTERFACE_IP}\")\n",
    "    print(\"\\n‚è≥ Capture in progress...\")\n",
    "    print(\"   (Visit websites, stream videos, etc. to generate traffic)\\n\")\n",
    "    \n",
    "    # Create flow aggregator\n",
    "    aggregator = FlowAggregator(flow_timeout=30)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Packet processing callback\n",
    "    def process_packet(packet):\n",
    "        aggregator.add_packet(packet)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if aggregator.packet_count % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            flows = len(aggregator.get_all_flows())\n",
    "            print(f\"   [{int(elapsed)}s] Packets: {aggregator.packet_count} | Flows: {flows}\", end='\\r')\n",
    "    \n",
    "    # Capture packets\n",
    "    try:\n",
    "        sniff(\n",
    "            iface=INTERFACE,\n",
    "            prn=process_packet,\n",
    "            timeout=duration,\n",
    "            store=False  # Don't store packets in memory\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Capture error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\\n‚úì Capture complete!\")\n",
    "    \n",
    "    # Analyze flows\n",
    "    flow_ids = aggregator.get_all_flows()\n",
    "    total_flows = len(flow_ids)\n",
    "    \n",
    "    print(f\"\\nüìä Analyzing {total_flows} flows...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    stats = {\n",
    "        'total_flows': total_flows,\n",
    "        'total_packets': aggregator.packet_count,\n",
    "        'total_alerts': 0,\n",
    "        'decisions': {'NORMAL': 0, 'ATTACK': 0},\n",
    "        'alert_levels': {'INFO': 0, 'WARNING': 0, 'CRITICAL': 0},\n",
    "        'protocols': {}\n",
    "    }\n",
    "    \n",
    "    for i, flow_id in enumerate(flow_ids):\n",
    "        try:\n",
    "            # Extract features and summary\n",
    "            features = aggregator.extract_features(flow_id)\n",
    "            flow_summary = aggregator.get_flow_summary(flow_id)\n",
    "            \n",
    "            # Predict\n",
    "            prediction = predict_flow(features, flow_summary)\n",
    "            results.append(prediction)\n",
    "            \n",
    "            # Update stats\n",
    "            stats['decisions'][prediction['final_decision']] += 1\n",
    "            stats['alert_levels'][prediction['alert_level']] += 1\n",
    "            \n",
    "            proto = prediction['protocol']\n",
    "            stats['protocols'][proto] = stats['protocols'].get(proto, 0) + 1\n",
    "            \n",
    "            if prediction['final_decision'] == 'ATTACK':\n",
    "                stats['total_alerts'] += 1\n",
    "            \n",
    "            # Progress\n",
    "            if (i + 1) % 10 == 0 or (i + 1) == total_flows:\n",
    "                print(f\"   Processed: {i+1}/{total_flows} flows\", end='\\r')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Error processing flow {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\\n‚úì Analysis complete!\")\n",
    "    \n",
    "    # Sort by alert level\n",
    "    alert_priority = {'CRITICAL': 0, 'WARNING': 1, 'INFO': 2}\n",
    "    results.sort(key=lambda x: alert_priority.get(x['alert_level'], 3))\n",
    "    \n",
    "    # Prepare JSON output\n",
    "    output = {\n",
    "        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'summary': {\n",
    "            'session_info': {\n",
    "                'interface': INTERFACE,\n",
    "                'interface_ip': INTERFACE_IP,\n",
    "                'capture_duration': duration,\n",
    "                'total_packets': stats['total_packets'],\n",
    "                'total_flows': stats['total_flows'],\n",
    "                'total_alerts': stats['total_alerts']\n",
    "            },\n",
    "            'statistics': {\n",
    "                'decisions': stats['decisions'],\n",
    "                'alert_levels': stats['alert_levels'],\n",
    "                'protocols': stats['protocols']\n",
    "            }\n",
    "        },\n",
    "        'recent_alerts': results[:50],  # Top 50 alerts\n",
    "        'top_suspicious': [r for r in results if r['final_decision'] == 'ATTACK'][:20]\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    def make_json_serializable(obj):\n",
    "        \"\"\"Convert numpy types, NaN, Inf to JSON-safe values\"\"\"\n",
    "        if isinstance(obj, (np.integer, np.floating)):\n",
    "            return obj.item()\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "        if isinstance(obj, list):\n",
    "            return [make_json_serializable(item) for item in obj]\n",
    "        if isinstance(obj, float):\n",
    "            if math.isnan(obj) or math.isinf(obj):\n",
    "                return 0.0\n",
    "        return obj\n",
    "\n",
    "# Save safe JSON\n",
    "    safe_output = make_json_serializable(output)\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(safe_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nüíæ Results saved to: {json_path} ({len(results)} flows)\")\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {json_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà DETECTION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Packets Captured: {stats['total_packets']:,}\")\n",
    "    print(f\"Flows Created: {stats['total_flows']:,}\")\n",
    "    print(f\"Alerts Generated: {stats['total_alerts']:,}\")\n",
    "    print(f\"\\nDecisions:\")\n",
    "    print(f\"  ‚Ä¢ Normal: {stats['decisions']['NORMAL']:,}\")\n",
    "    print(f\"  ‚Ä¢ Attack: {stats['decisions']['ATTACK']:,}\")\n",
    "    print(f\"\\nAlert Levels:\")\n",
    "    print(f\"  ‚Ä¢ Critical: {stats['alert_levels']['CRITICAL']:,}\")\n",
    "    print(f\"  ‚Ä¢ Warning: {stats['alert_levels']['WARNING']:,}\")\n",
    "    print(f\"  ‚Ä¢ Info: {stats['alert_levels']['INFO']:,}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Run single capture\n",
    "print(\"\\nüöÄ Starting single capture session...\\n\")\n",
    "result = capture_and_analyze(duration=60)\n",
    "\n",
    "if result:\n",
    "    print(\"\\n‚úÖ SUCCESS! Live detection completed.\")\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   1. Run the Flask dashboard: python ../dashboard/app.py\")\n",
    "    print(\"   2. Open browser: http://localhost:5000\")\n",
    "    print(\"   3. View live detection results!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Capture failed. Check interface and permissions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f38615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Continuous Monitoring Mode (Optional)\n",
    "print(\"=\"*70)\n",
    "print(\"üîÑ CONTINUOUS MONITORING MODE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis cell runs continuous monitoring (updates every 60 seconds)\")\n",
    "print(\"‚ö†Ô∏è  WARNING: This will run indefinitely. Use 'Interrupt Kernel' to stop.\\n\")\n",
    "\n",
    "def continuous_monitoring(capture_duration=60, update_interval=60):\n",
    "    \"\"\"\n",
    "    Run continuous monitoring with periodic updates\n",
    "    Args:\n",
    "        capture_duration: Duration of each capture session (seconds)\n",
    "        update_interval: Time between capture sessions (seconds)\n",
    "    \"\"\"\n",
    "    session_count = 0\n",
    "    \n",
    "    print(f\"üîÑ Continuous monitoring started\")\n",
    "    print(f\"   Capture duration: {capture_duration}s\")\n",
    "    print(f\"   Update interval: {update_interval}s\")\n",
    "    print(f\"\\n   Press Ctrl+C or use 'Interrupt Kernel' to stop\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            session_count += 1\n",
    "            print(f\"\\nüîπ SESSION #{session_count} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            # Capture and analyze\n",
    "            result = capture_and_analyze(duration=capture_duration)\n",
    "            \n",
    "            if result:\n",
    "                print(f\"\\n‚úì Session #{session_count} complete. JSON updated.\")\n",
    "                print(f\"   Next update in {update_interval} seconds...\\n\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Session #{session_count} failed. Retrying...\\n\")\n",
    "            \n",
    "            # Wait before next capture\n",
    "            time.sleep(update_interval)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"üõë MONITORING STOPPED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total sessions completed: {session_count}\")\n",
    "        print(f\"Last update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"\\n‚úì JSON file saved. Dashboard can still access data.\")\n",
    "\n",
    "# Uncomment the line below to start continuous monitoring\n",
    "# continuous_monitoring(capture_duration=60, update_interval=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
