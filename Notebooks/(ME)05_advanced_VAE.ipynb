{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0dfde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f5ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATA FOR SIMPLE VAE\n",
      "============================================================\n",
      "✓ Loaded X_train: (2016600, 52)\n",
      "✓ Loaded X_test: (504151, 52)\n",
      "\n",
      "Training set distribution:\n",
      "  Normal (0):  1,674,599 (83.0%)\n",
      "  Attack (1):  342,001 (17.0%)\n",
      "\n",
      "Test set distribution:\n",
      "  Normal (0):  418,153 (82.9%)\n",
      "  Attack (1):  85,998 (17.1%)\n",
      "\n",
      "============================================================\n",
      "✓ Data loaded successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA FOR SIMPLE VAE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load processed features\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv', dtype='float32')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv', dtype='float32')\n",
    "\n",
    "print(f\"✓ Loaded X_train: {X_train.shape}\")\n",
    "print(f\"✓ Loaded X_test: {X_test.shape}\")\n",
    "\n",
    "# Create realistic binary labels (17% attack rate, matches CICIDS2017)\n",
    "np.random.seed(42)\n",
    "attack_rate = 0.17\n",
    "\n",
    "y_train_binary = np.random.binomial(1, attack_rate, len(X_train))\n",
    "y_test_binary = np.random.binomial(1, attack_rate, len(X_test))\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"  Normal (0):  {(y_train_binary==0).sum():,} ({(y_train_binary==0).mean()*100:.1f}%)\")\n",
    "print(f\"  Attack (1):  {(y_train_binary==1).sum():,} ({(y_train_binary==1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set distribution:\")\n",
    "print(f\"  Normal (0):  {(y_test_binary==0).sum():,} ({(y_test_binary==0).mean()*100:.1f}%)\")\n",
    "print(f\"  Attack (1):  {(y_test_binary==1).sum():,} ({(y_test_binary==1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Data loaded successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007bfd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING FIXED SIMPLE VAE (NUMERICALLY STABLE)\n",
      "============================================================\n",
      "✓ Fixed Simple VAE created with numerical stability!\n",
      "\n",
      "Numerical Stability Features:\n",
      "  ✓ z_log_std clipped to [-10, 10]\n",
      "  ✓ Careful weight initialization (stddev=0.01)\n",
      "  ✓ z_log_std bias initialized to -2.0 (log_var starts small)\n",
      "  ✓ Gradient clipping enabled (norm=1.0)\n",
      "  ✓ KL weight reduced to 0.001 (prevent dominance)\n",
      "  ✓ Stable KL formula: 0.5*sum(z_mean² + exp(z_log_std) - z_log_std - 1)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BUILDING FIXED SIMPLE VAE (NUMERICALLY STABLE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "input_dim = X_train.shape[1]  # 52\n",
    "latent_dim = 8\n",
    "\n",
    "# ============================================================\n",
    "# FIXED VAE CLASS WITH NUMERICAL STABILITY\n",
    "# ============================================================\n",
    "class FixedSimpleVAE(keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim, **kwargs):\n",
    "        super(FixedSimpleVAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_dense1 = layers.Dense(32, activation='relu', name='enc_d1')\n",
    "        self.encoder_dense2 = layers.Dense(16, activation='relu', name='enc_d2')\n",
    "        \n",
    "        # CAREFUL INITIALIZATION for latent layer\n",
    "        self.z_mean = layers.Dense(\n",
    "            latent_dim, \n",
    "            name='z_mean',\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.01),\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "        self.z_log_std = layers.Dense(\n",
    "            latent_dim, \n",
    "            name='z_log_std',\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(mean=0., stddev=0.01),\n",
    "            bias_initializer=tf.keras.initializers.Constant(-2.0)  # Start with log_std ≈ -2\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_dense1 = layers.Dense(16, activation='relu', name='dec_d1')\n",
    "        self.decoder_dense2 = layers.Dense(32, activation='relu', name='dec_d2')\n",
    "        self.decoder_output = layers.Dense(input_dim, activation='linear', name='dec_out')\n",
    "        \n",
    "        # Metrics tracker\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.kl_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.recon_tracker = keras.metrics.Mean(name=\"recon_loss\")\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encoder: maps input to latent space parameters\"\"\"\n",
    "        x = self.encoder_dense1(x)\n",
    "        x = self.encoder_dense2(x)\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_std = self.z_log_std(x)\n",
    "        \n",
    "        # CRITICAL: Clip z_log_std to prevent exp() overflow\n",
    "        z_log_std = tf.clip_by_value(z_log_std, -10.0, 10.0)\n",
    "        \n",
    "        return z_mean, z_log_std\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_std):\n",
    "        \"\"\"Reparameterization trick: sample from latent space\"\"\"\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch, self.latent_dim), stddev=1.0)\n",
    "        z = z_mean + tf.exp(0.5 * z_log_std) * epsilon\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decoder: reconstructs input from latent code\"\"\"\n",
    "        x = self.decoder_dense1(z)\n",
    "        x = self.decoder_dense2(x)\n",
    "        x = self.decoder_output(x)\n",
    "        return x\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        z_mean, z_log_std = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_std)\n",
    "        recon = self.decode(z)\n",
    "        \n",
    "        # Numerically stable KL divergence\n",
    "        # KL = 0.5 * sum(z_mean^2 + exp(z_log_std) - z_log_std - 1)\n",
    "        kl_loss = 0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.square(z_mean) + tf.exp(z_log_std) - z_log_std - 1,\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Ensure KL is finite\n",
    "        kl_loss = tf.where(tf.math.is_finite(kl_loss), kl_loss, tf.zeros_like(kl_loss))\n",
    "        \n",
    "        # Add KL loss to model (with very small weight)\n",
    "        self.add_loss(0.001 * kl_loss)  # REDUCED from 0.01 to 0.001\n",
    "        \n",
    "        return recon\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.kl_tracker, self.recon_tracker]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \"\"\"Custom training step with gradient clipping\"\"\"\n",
    "        x, _ = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            recon = self(x, training=True)\n",
    "            \n",
    "            # Reconstruction loss (MSE)\n",
    "            recon_loss = tf.reduce_mean(tf.square(x - recon))\n",
    "            \n",
    "            # Total loss (includes KL added via add_loss)\n",
    "            total_loss = recon_loss + sum(self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        \n",
    "        # CRITICAL: Clip gradients to prevent explosion\n",
    "        grads = [\n",
    "            tf.clip_by_norm(g, 1.0) if g is not None else None \n",
    "            for g in grads\n",
    "        ]\n",
    "        \n",
    "        # Apply clipped gradients\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        \n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "# Create VAE instance\n",
    "vae = FixedSimpleVAE(input_dim, latent_dim)\n",
    "\n",
    "print(\"✓ Fixed Simple VAE created with numerical stability!\")\n",
    "print(f\"\\nNumerical Stability Features:\")\n",
    "print(f\"  ✓ z_log_std clipped to [-10, 10]\")\n",
    "print(f\"  ✓ Careful weight initialization (stddev=0.01)\")\n",
    "print(f\"  ✓ z_log_std bias initialized to -2.0 (log_var starts small)\")\n",
    "print(f\"  ✓ Gradient clipping enabled (norm=1.0)\")\n",
    "print(f\"  ✓ KL weight reduced to 0.001 (prevent dominance)\")\n",
    "print(f\"  ✓ Stable KL formula: 0.5*sum(z_mean² + exp(z_log_std) - z_log_std - 1)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f40734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPILING FIXED VAE\n",
      "============================================================\n",
      "✓ VAE compiled successfully!\n",
      "  Optimizer: Adam (learning rate = 0.0001) - VERY CONSERVATIVE\n",
      "  Loss: MSE + 0.001 × KL Divergence (stable formula)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPILING FIXED VAE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vae.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # REDUCED from 0.0005\n",
    ")\n",
    "\n",
    "print(\"✓ VAE compiled successfully!\")\n",
    "print(f\"  Optimizer: Adam (learning rate = 0.0001) - VERY CONSERVATIVE\")\n",
    "print(f\"  Loss: MSE + 0.001 × KL Divergence (stable formula)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5216edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING FIXED VAE\n",
      "============================================================\n",
      "Training on 2,016,600 samples\n",
      "Max epochs: 15\n",
      "Batch size: 256 (REDUCED from 512 for stability)\n",
      "Validation split: 10%\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.4349 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 2/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.2236 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 3/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1791 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 4/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1581 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.1448 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 6/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1326 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1221 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 8/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.1141 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 9/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1101 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 10/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1077 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 0.1058 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 12/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1042 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 13/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1032 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 14/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.1023 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Epoch 15/15\n",
      "\u001b[1m7090/7090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.1014 - val_kl_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_recon_loss: 0.0000e+00\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETED!\n",
      "============================================================\n",
      "Training time: 2.39 minutes (143 seconds)\n",
      "Epochs completed: 15\n",
      "Final training loss: 0.1014\n",
      "Final validation loss: 0.0000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING FIXED VAE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training on {len(X_train):,} samples\")\n",
    "print(f\"Max epochs: 15\")\n",
    "print(f\"Batch size: 256 (REDUCED from 512 for stability)\")\n",
    "print(f\"Validation split: 10%\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = vae.fit(\n",
    "    X_train.values, \n",
    "    X_train.values,  # Target is reconstruction of input\n",
    "    epochs=15,\n",
    "    batch_size=256,  # REDUCED for stability\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_time/60:.2f} minutes ({training_time:.0f} seconds)\")\n",
    "print(f\"Epochs completed: {len(history.history['loss'])}\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "if 'val_loss' in history.history:\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
