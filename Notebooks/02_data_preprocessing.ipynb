{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5cc204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b60807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2,520,751 rows, 53 columns\n",
      "Binary labels created successfully!\n",
      "  - Normal Traffic (0): 2,095,057 (83.11%)\n",
      "  - Attack (1): 425,694 (16.89%)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw/cicids2017_cleaned.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Create binary labels immediately\n",
    "df['Binary_Label'] = df['Attack Type'].apply(lambda x: 0 if x == 'Normal Traffic' else 1)\n",
    "\n",
    "print(f\"Binary labels created successfully!\")\n",
    "print(f\"  - Normal Traffic (0): {(df['Binary_Label']==0).sum():,} ({(df['Binary_Label']==0).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"  - Attack (1): {(df['Binary_Label']==1).sum():,} ({(df['Binary_Label']==1).sum()/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f3ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating features and target...\n",
      "============================================================\n",
      "Features (X): (2520751, 52)\n",
      "Target (y): (2520751,)\n",
      "\n",
      "Feature columns: ['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Length of Fwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min']...\n",
      "Total features: 52\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "print(\"Separating features and target...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove the original 'Attack Type' column and keep only Binary_Label\n",
    "X = df.drop(['Attack Type', 'Binary_Label'], axis=1)\n",
    "y = df['Binary_Label']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"\\nFeature columns: {X.columns.tolist()[:10]}...\")  # Show first 10\n",
    "print(f\"Total features: {len(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329f7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking feature correlations...\n",
      "============================================================\n",
      "Features with correlation > 0.95: 12\n",
      "Highly correlated features: ['Fwd Packet Length Std', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Fwd IAT Total', 'Fwd IAT Max', 'Fwd Packets/s', 'Packet Length Std', 'Average Packet Size', 'Subflow Fwd Bytes', 'Idle Mean']\n",
      "\n",
      "⚠ Note: We'll handle these during feature selection later\n"
     ]
    }
   ],
   "source": [
    "# Check for highly correlated features\n",
    "print(\"Checking feature correlations...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X.corr().abs()\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "print(f\"Features with correlation > 0.95: {len(high_corr_features)}\")\n",
    "if len(high_corr_features) > 0:\n",
    "    print(f\"Highly correlated features: {high_corr_features[:10]}\")  # Show first 10\n",
    "    print(\"\\n⚠ Note: We'll handle these during feature selection later\")\n",
    "else:\n",
    "    print(\"✓ No highly correlated features found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a3153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and test sets...\n",
      "============================================================\n",
      "Training set: 2,016,600 samples\n",
      "Testing set: 504,151 samples\n",
      "\n",
      "Class distribution in training set:\n",
      "  - Normal Traffic (0): 1,676,045 (83.11%)\n",
      "  - Attack (1): 340,555 (16.89%)\n",
      "\n",
      "Class distribution in testing set:\n",
      "  - Normal Traffic (0): 419,012 (83.11%)\n",
      "  - Attack (1): 85,139 (16.89%)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets with stratification\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 80% train, 20% test\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y          # Maintains class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(f\"  - Normal Traffic (0): {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  - Attack (1): {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(f\"  - Normal Traffic (0): {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  - Attack (1): {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f98d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Feature Scaling (Standardization)...\n",
      "============================================================\n",
      "✓ Feature scaling completed!\n",
      "\n",
      "Scaled training set shape: (2016600, 52)\n",
      "Scaled testing set shape: (504151, 52)\n",
      "\n",
      "Verification (first 3 features):\n",
      "Mean values: [-5.02834245e-17  7.49794179e-18  1.03149552e-18]\n",
      "Std values: [1.00000025 1.00000025 1.00000025]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling using StandardScaler\n",
    "print(\"Applying Feature Scaling (Standardization)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# IMPORTANT: Fit ONLY on training data to prevent data leakage\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform, don't fit\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"✓ Feature scaling completed!\")\n",
    "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify scaling worked (mean should be ~0, std should be ~1)\n",
    "print(\"\\nVerification (first 3 features):\")\n",
    "print(f\"Mean values: {X_train_scaled.iloc[:, :3].mean().values}\")\n",
    "print(f\"Std values: {X_train_scaled.iloc[:, :3].std().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d068e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data...\n",
      "============================================================\n",
      "✓ Directories created/verified\n",
      "✓ Preprocessed data saved successfully!\n",
      "\n",
      "Saved files:\n",
      "  - ../data/processed/X_train.csv\n",
      "  - ../data/processed/y_train.csv\n",
      "  - ../data/processed/X_test.csv\n",
      "  - ../data/processed/y_test.csv\n",
      "  - ../saved_models/scaler.pkl (for future predictions)\n",
      "\n",
      "File sizes:\n",
      "  - X_train.csv: 2036.98 MB\n",
      "  - X_test.csv: 509.24 MB\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data\n",
    "print(\"Saving preprocessed data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../saved_models', exist_ok=True)\n",
    "\n",
    "print(\"✓ Directories created/verified\")\n",
    "\n",
    "# Save training data\n",
    "X_train_scaled.to_csv('../data/processed/X_train.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "\n",
    "# Save testing data\n",
    "X_test_scaled.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../saved_models/scaler.pkl')\n",
    "\n",
    "print(\"✓ Preprocessed data saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - ../data/processed/X_train.csv\")\n",
    "print(\"  - ../data/processed/y_train.csv\")\n",
    "print(\"  - ../data/processed/X_test.csv\")\n",
    "print(\"  - ../data/processed/y_test.csv\")\n",
    "print(\"  - ../saved_models/scaler.pkl (for future predictions)\")\n",
    "\n",
    "# Verify file sizes\n",
    "print(\"\\nFile sizes:\")\n",
    "print(f\"  - X_train.csv: {os.path.getsize('../data/processed/X_train.csv') / (1024*1024):.2f} MB\")\n",
    "print(f\"  - X_test.csv: {os.path.getsize('../data/processed/X_test.csv') / (1024*1024):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
